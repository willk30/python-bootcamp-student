{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Block 2: Essential Data Wrangling with NumPy & Pandas\n",
    "\n",
    "**Python Module for Incoming ISE & OR PhD Students**  \n",
    "Instructor: Will Kirschenman | August 7, 2025 | 10:00 AM - 10:50 AM\n",
    "\n",
    "---\n",
    "\n",
    "## Welcome to Block 2! üê∫\n",
    "\n",
    "In Block 1, we learned the programming mindset and Python basics. Now we're diving into the real power of Python for data science: **NumPy** and **Pandas**. These are the tools that make Python the go-to language for data analysis in research.\n",
    "\n",
    "By the end of this block, you'll be able to:\n",
    "- Work with NumPy arrays for efficient numerical computations\n",
    "- Load, explore, and manipulate data with Pandas DataFrames\n",
    "- Clean messy real-world data (the kind you'll encounter in research)\n",
    "- Prepare a clean dataset for machine learning (Block 3!)\n",
    "\n",
    "**Our Mission**: We have a dataset of NC State PhD student research productivity that's... well, let's just say it needs some TLC. We'll learn NumPy and Pandas while turning this messy data into something beautiful and analysis-ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Package Installation & Setup\n",
    "# Run this cell ONLY if you encounter import errors\n",
    "# Most packages are pre-installed in Google Colab\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_package(package_name):\n",
    "    \"\"\"Install a package using pip if not already installed\"\"\"\n",
    "    try:\n",
    "        __import__(package_name)\n",
    "        print(f\"‚úÖ {package_name} already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"üì¶ Installing {package_name}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\n",
    "        print(f\"‚úÖ {package_name} installed successfully\")\n",
    "\n",
    "# Core packages used in this notebook (Block 2: Data Wrangling)\n",
    "required_packages = [\n",
    "    'numpy',\n",
    "    'pandas', \n",
    "    'matplotlib'\n",
    "]\n",
    "\n",
    "print(\"üîç Checking required packages for Block 2...\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "for package in required_packages:\n",
    "    install_package(package)\n",
    "\n",
    "print(\"\\nüéâ All packages ready! You can now run all cells without import errors.\")\n",
    "print(\"üí° Tip: In Google Colab, most packages are pre-installed, so you likely won't need to install anything!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Setup: Import Our Tools\n",
    "\n",
    "Let's start by importing the libraries we'll need. In Google Colab, these are already installed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Creating Our Dataset\n",
    "\n",
    "Since we're working in Google Colab, let's create our synthetic dataset right here. This represents research productivity data for PhD students at NC State - but it's messy, just like real data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2025)\n",
    "random.seed(2025)\n",
    "\n",
    "# Generate synthetic PhD student data\n",
    "def create_phd_dataset():\n",
    "    \"\"\"\n",
    "    Create a realistic but messy dataset of PhD student research productivity\n",
    "    \"\"\"\n",
    "    # For reproducible results, uncomment the following two lines\n",
    "    # np.random.seed(42)\n",
    "    # random.seed(42)\n",
    "    \n",
    "    departments = {\n",
    "        'ISE': ['ISE', 'I.S.E.', 'Industrial Systems', 'Industrial & Systems Engineering'],\n",
    "        'OR': ['OR', 'O.R.', 'Operations Research', 'Operations Res.'],\n",
    "        'CSC': ['CSC', 'Computer Science', 'CS', 'Comp Sci'],\n",
    "        'ECE': ['ECE', 'Electrical & Computer Engineering', 'E.C.E.', 'EE'],\n",
    "        'MAE': ['MAE', 'Mechanical & Aerospace Engineering', 'M.A.E.', 'Mechanical']\n",
    "    }\n",
    "    \n",
    "    print(\"Type of departments:\", type(departments))  # Ensure departments is a dictionary\n",
    "    \n",
    "    n_students = 280\n",
    "    data = []\n",
    "    \n",
    "    for i in range(n_students):\n",
    "        # Basic info\n",
    "        student_id = f\"PhD_{i+1:03d}\"\n",
    "        \n",
    "        # Department (with inconsistent formatting)\n",
    "        dept_clean = random.choice(list(departments.keys()))\n",
    "        dept_messy = random.choice(departments[dept_clean])\n",
    "        \n",
    "        # Years in program (1-7, with most being 2-4)\n",
    "        years = np.random.choice([1, 2, 3, 4, 5, 6, 7], p=[0.1, 0.2, 0.25, 0.2, 0.15, 0.05, 0.05])\n",
    "        \n",
    "        # Papers published (correlated with years, but with variation)\n",
    "        base_papers = max(0, years - 1 + np.random.normal(0, 1.5))\n",
    "        papers = max(0, int(base_papers))\n",
    "        \n",
    "        # Conferences attended (somewhat correlated with papers)\n",
    "        conferences = max(0, int(papers * 0.8 + np.random.normal(0, 1)))\n",
    "        \n",
    "        # Coffee consumption (PhD students love coffee, with some outliers)\n",
    "        coffee_base = np.random.normal(3.5, 1.5)\n",
    "        coffee = max(0, coffee_base)\n",
    "        \n",
    "        # Hours in Hunt Library (varies widely, some night owls)\n",
    "        hunt_hours = max(0, np.random.normal(25, 12))\n",
    "        \n",
    "        # Advisor meetings per month (1-8, mostly 2-4)\n",
    "        advisor_meetings = max(1, int(np.random.normal(3, 1.5)))\n",
    "        \n",
    "        # Stress level (1-10, correlated with years and workload)\n",
    "        stress_base = 3 + years * 0.5 + papers * 0.3 + np.random.normal(0, 1.5)\n",
    "        stress = max(1, min(10, stress_base))\n",
    "        \n",
    "        # Funding amount (varies by department and year)\n",
    "        funding_base = 25000 + years * 1000 + np.random.normal(0, 3000)\n",
    "        funding = max(15000, funding_base)\n",
    "        \n",
    "        # Distance from campus (most live close, some commute)\n",
    "        distance = max(0.1, np.random.exponential(3))\n",
    "        \n",
    "        # Add some missing values\n",
    "        if random.random() < 0.08:  # 8% missing coffee data\n",
    "            coffee = np.nan\n",
    "        if random.random() < 0.05:  # 5% missing stress data\n",
    "            stress = np.nan\n",
    "        if random.random() < 0.03:  # 3% missing funding data\n",
    "            funding = np.nan\n",
    "        \n",
    "        data.append({\n",
    "            'student_id': student_id,\n",
    "            'department': dept_messy,\n",
    "            'years_in_program': years,\n",
    "            'papers_published': papers,\n",
    "            'conferences_attended': conferences,\n",
    "            'coffee_cups_per_day': coffee,\n",
    "            'hours_in_hunt_library_per_week': hunt_hours,\n",
    "            'advisor_meetings_per_month': advisor_meetings,\n",
    "            'stress_level': stress,\n",
    "            'funding_amount': funding,\n",
    "            'distance_from_campus_miles': distance\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Add some duplicate rows (for cleaning exercise)\n",
    "    duplicates = df.sample(n=8, random_state=42)\n",
    "    df = pd.concat([df, duplicates], ignore_index=True)\n",
    "    \n",
    "    # Add some extreme outliers for coffee consumption\n",
    "    outlier_indices = np.random.choice(df.index, size=5, replace=False)\n",
    "    df.loc[outlier_indices, 'coffee_cups_per_day'] = np.random.uniform(15, 25, size=5)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create our dataset\n",
    "df_raw = create_phd_dataset()\n",
    "\n",
    "print(f\"üéØ Dataset created with {len(df_raw)} rows\")\n",
    "print(f\"üìä Columns: {list(df_raw.columns)}\")\n",
    "print(f\"‚ùå Missing values: {df_raw.isnull().sum().sum()}\")\n",
    "print(f\"üîÑ Duplicate rows: {df_raw.duplicated().sum()}\")\n",
    "print(\"\\nüîç Let's peek at the first few rows...\")\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## üò¨ Houston, We Have Problems...\n",
    "\n",
    "Look at that data! It's messy:\n",
    "- Missing values (NaN)\n",
    "- Duplicate rows\n",
    "- Inconsistent department names\n",
    "- Probably some outliers too\n",
    "\n",
    "**Don't panic!** This is exactly the kind of data you'll encounter in real research. By the end of this block, we'll have it sparkling clean and ready for analysis.\n",
    "\n",
    "But first, let's learn the tools we need..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: NumPy Fundamentals üî¢\n",
    "\n",
    "## Why NumPy?\n",
    "\n",
    "NumPy (Numerical Python) is the foundation of the Python data science ecosystem. It provides:\n",
    "- **Speed**: Operations are implemented in C, making them much faster than pure Python\n",
    "- **Memory efficiency**: Arrays use less memory than Python lists\n",
    "- **Vectorization**: Perform operations on entire arrays at once\n",
    "\n",
    "Let's see the speed difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Create large datasets\n",
    "n_elements = 1000000\n",
    "python_list = list(range(n_elements))\n",
    "numpy_array = np.array(python_list)\n",
    "\n",
    "# Time Python list operation\n",
    "start_time = time.perf_counter()\n",
    "python_result = [x * 2 for x in python_list]\n",
    "python_time = time.perf_counter() - start_time\n",
    "\n",
    "# Time NumPy operation\n",
    "start_time = time.perf_counter()\n",
    "numpy_result = numpy_array * 2\n",
    "numpy_time = time.perf_counter() - start_time\n",
    "\n",
    "print(\"--- Performance ---\")\n",
    "print(f\"üêå Python list: {python_time:.4f} seconds\")\n",
    "print(f\"üöÄ NumPy array: {numpy_time:.4f} seconds\")\n",
    "print(f\"‚ö° NumPy is {python_time/numpy_time:.1f}x faster!\")\n",
    "\n",
    "# --- Memory Usage Comparison ---\n",
    "\n",
    "# Python list memory calculation (more accurately, the memory of the list object itself\n",
    "# plus the memory of the integer objects it points to)\n",
    "# This is a good approximation without iterating over every element.\n",
    "# On a 64-bit system, a standard integer object is 28 bytes.\n",
    "# We also have to account for the list object itself and the pointers.\n",
    "# A simpler, more direct approach is to show the difference\n",
    "# in how memory is stored.\n",
    "python_list_mem_approx = sys.getsizeof(python_list) + n_elements * sys.getsizeof(python_list[0])\n",
    "numpy_array_mem = numpy_array.nbytes\n",
    "\n",
    "print(\"\\n--- Memory Usage ---\")\n",
    "print(f\"üß† Python list memory: {python_list_mem_approx / 1024 / 1024:.2f} MB (approx.)\")\n",
    "print(f\"üß† NumPy array memory: {numpy_array_mem / 1024 / 1024:.2f} MB\")\n",
    "print(f\"üí° NumPy uses {python_list_mem_approx / numpy_array_mem:.1f}x less memory!\")\n",
    "\n",
    "# --- Conclusion ---\n",
    "print(\"\\nThis is why we use NumPy for numerical computing! It's both faster and more memory-efficient.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Creating NumPy Arrays\n",
    "\n",
    "There are several ways to create NumPy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From a Python list\n",
    "coffee_consumption = np.array([3.2, 4.1, 2.8, 5.0, 3.7])\n",
    "print(f\"‚òï Coffee consumption: {coffee_consumption}\")\n",
    "print(f\"Type: {type(coffee_consumption)}\") # numpy n-dimensional array object\n",
    "\n",
    "# Create arrays with specific values\n",
    "zeros = np.zeros(5)  # Array of zeros\n",
    "ones = np.ones(3)    # Array of ones\n",
    "range_array = np.arange(1, 8)  # Range from 1 to 7 (like range() but NumPy)\n",
    "\n",
    "print(f\"\\nüî¢ Zeros: {zeros}\")\n",
    "print(f\"üî¢ Ones: {ones}\")\n",
    "print(f\"üî¢ Range (PhD years): {range_array}\")\n",
    "\n",
    "# Random arrays (useful for simulations)\n",
    "random_stress = np.random.normal(5, 2, 10)  # Mean=5, std=2, 10 samples\n",
    "print(f\"\\nüò∞ Random stress levels: {random_stress}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create the following arrays using NumPy functions:\n",
    "\n",
    "# 1. Create an array of 10 zeros to represent baseline measurements\n",
    "# Hint: Use a function that creates an array of a specified size and fills it with a single value.\n",
    "baseline_measurements = ???\n",
    "\n",
    "# 2. Create an array of survey scores from 1 to 5 (inclusive)\n",
    "# Hint: Use a function similar to Python's `range()` but for NumPy arrays.\n",
    "survey_scale = ???\n",
    "\n",
    "# 3. Create an array of 8 ones to represent control group results\n",
    "# Hint: This is similar to creating an array of zeros, but with a different fill value.\n",
    "control_group = ???\n",
    "\n",
    "# 4. Generate 15 random experiment results between 0 and 100\n",
    "# Hint: try `np.random.randint()` or `np.random.uniform()`, similar to the random stress example above.\n",
    "experiment_results = ???\n",
    "\n",
    "# 5. Create an array with values [2, 4, 6, 8, 10] using arange\n",
    "# Hint: The `arange` function can take an optional step size.\n",
    "even_numbers = ???\n",
    "\n",
    "print(\"‚úÖ Array Creation Exercise:\")\n",
    "print(f\"Baseline measurements: {baseline_measurements}\")\n",
    "print(f\"Survey scale: {survey_scale}\")\n",
    "print(f\"Control group: {control_group}\")\n",
    "print(f\"Experiment results: {experiment_results}\")\n",
    "print(f\"Even numbers: {even_numbers}\")\n",
    "\n",
    "# Check your work:\n",
    "print(f\"\\nüîç Verification:\")\n",
    "print(f\"Baseline has {len(baseline_measurements)} zeros: {np.all(baseline_measurements == 0)}\")\n",
    "print(f\"Survey scale goes 1-5: {np.array_equal(survey_scale, [1, 2, 3, 4, 5])}\")\n",
    "print(f\"Control group has {len(control_group)} ones: {np.all(control_group == 1)}\")\n",
    "print(f\"Experiment results shape: {experiment_results.shape}\")\n",
    "print(f\"Even numbers are correct: {np.array_equal(even_numbers, [2, 4, 6, 8, 10])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Basic NumPy Operations\n",
    "\n",
    "NumPy shines with vectorized operations - you can perform calculations on entire arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's work with some PhD student data\n",
    "papers_per_year = np.array([0, 1, 2, 1, 3, 2, 1])  # Papers published each year\n",
    "conference_costs = np.array([1200, 1500, 800, 2000, 1100])  # Conference registration costs\n",
    "\n",
    "# Mathematical operations\n",
    "total_papers = np.sum(papers_per_year)\n",
    "avg_papers = np.mean(papers_per_year)\n",
    "max_papers = np.max(papers_per_year)\n",
    "\n",
    "print(f\"üìÑ Total papers published: {total_papers}\")\n",
    "print(f\"üìä Average papers per year: {avg_papers:.2f}\")\n",
    "print(f\"üèÜ Best year (max papers): {max_papers}\")\n",
    "\n",
    "# Vectorized operations\n",
    "papers_doubled = papers_per_year * 2  # What if we were twice as productive?\n",
    "print(f\"\\nüöÄ If we doubled our productivity: {papers_doubled}\")\n",
    "\n",
    "# Statistical operations\n",
    "print(f\"\\nüí∞ Conference costs statistics:\")\n",
    "print(f\"   Mean: ${np.mean(conference_costs):.2f}\")\n",
    "print(f\"   Median: ${np.median(conference_costs):.2f}\")\n",
    "print(f\"   Std Dev: ${np.std(conference_costs):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## üéØ Mini Exercise: Research Metrics Analysis\n",
    "\n",
    "Now that you've learned the statistical functions, practice using them with research data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated research data\n",
    "experiment_scores = np.array([85, 92, 78, 95, 88, 91, 79, 86, 93, 84, 77, 89])\n",
    "control_scores = np.array([82, 87, 75, 90, 85, 88, 76, 83, 89, 81])\n",
    "\n",
    "# TODO: Calculate the following statistics:\n",
    "\n",
    "# 1. Mean score for each group\n",
    "experiment_mean = ???\n",
    "control_mean = ???\n",
    "\n",
    "# 2. Standard deviation for each group  \n",
    "experiment_std = ???\n",
    "control_std = ???\n",
    "\n",
    "# 3. The difference between group means\n",
    "mean_difference = ???\n",
    "\n",
    "# 4. Find the highest and lowest scores in the experiment group\n",
    "experiment_max = ???\n",
    "experiment_min = ???\n",
    "\n",
    "# 5. Count how many experiment scores are above 85\n",
    "# Hint: A comparison like `experiment_scores > 85` will give you a boolean array.\n",
    "# What is a fast way to count the `True` values in a NumPy array?\n",
    "above_85_count = ???\n",
    "\n",
    "print(\"üìä Research Analysis Results:\")\n",
    "print(f\"Experiment group mean: {experiment_mean:.2f}\")\n",
    "print(f\"Control group mean: {control_mean:.2f}\")\n",
    "print(f\"Difference in means: {mean_difference:.2f}\")\n",
    "print(f\"Experiment std dev: {experiment_std:.2f}\")\n",
    "print(f\"Control std dev: {control_std:.2f}\")\n",
    "print(f\"Experiment range: {experiment_min} to {experiment_max}\")\n",
    "print(f\"Experiment scores above 85: {above_85_count} out of {len(experiment_scores)}\")\n",
    "\n",
    "# Bonus: Which group performed better?\n",
    "if experiment_mean > control_mean:\n",
    "    print(\"üèÜ The experiment group performed better!\")\n",
    "else:\n",
    "    print(\"üèÜ The control group performed better!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## üéØ Mini Exercise: Hunt Library Hours\n",
    "\n",
    "Analyze the Hunt Library hours data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Hunt Library hours data (removing NaN values)\n",
    "hunt_hours = df_raw['hours_in_hunt_library_per_week'].dropna().values\n",
    "\n",
    "# TODO: Calculate and print these statistics:\n",
    "# 1. Mean hours per week\n",
    "# 2. How many students spend more than 40 hours/week in Hunt Library\n",
    "# 3. What percentage of students basically live in Hunt Library (>50 hours/week)?\n",
    "\n",
    "# Your code here:\n",
    "mean_hours = ???\n",
    "over_40_hours = ???\n",
    "over_50_hours = ???\n",
    "percentage_over_50 = ???\n",
    "\n",
    "print(f\"üìö Hunt Library analysis for {len(hunt_hours)} students:\")\n",
    "print(f\"   Mean hours per week: {mean_hours:.2f}\")\n",
    "print(f\"   Students spending >40 hours/week: {over_40_hours}\")\n",
    "print(f\"   Students basically living in Hunt Library (>50 hours/week): {over_50_hours}\")\n",
    "print(f\"   Percentage living in Hunt Library: {percentage_over_50:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Pandas - Your Data Analysis Best Friend üêº\n",
    "\n",
    "## NumPy vs Pandas: When to Use What?\n",
    "\n",
    "- **NumPy**: Best for numerical computations, homogeneous data, mathematical operations\n",
    "- **Pandas**: Best for mixed data types, labeled data, data cleaning, real-world datasets\n",
    "\n",
    "Pandas is built on top of NumPy, so you get the speed benefits plus much more functionality!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## Meet the DataFrame\n",
    "\n",
    "The DataFrame is like a supercharged spreadsheet. Let's explore our PhD student data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start fresh with our raw data\n",
    "df = df_raw.copy()  # Make a copy so we don't mess up the original\n",
    "\n",
    "# df.shape shows the number of rows and columns; df.columns lists the column names\n",
    "print(\"üìä DataFrame Shape:\", df.shape)\n",
    "print(\"\\nüìã Column Names:\")\n",
    "for i, col in enumerate(df.columns):\n",
    "    print(f\"   {i}: {col}\")\n",
    "\n",
    "print(\"\\nüîç Data Types:\")\n",
    "print(df.dtypes) # df.dtypes shows the data type of each column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## Essential DataFrame Methods\n",
    "\n",
    "These are the methods you'll use constantly:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    ".head() shows the first few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First few rows\n",
    "print(\"üîù First 5 rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    ".tail() shows the last few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last few rows\n",
    "print(\"\\nüîö Last 3 rows:\")\n",
    "print(df.tail(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    ".info() gives a summary of the DataFrame including non-null counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General information about the dataset\n",
    "print(\"‚ÑπÔ∏è  Dataset Info:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    ".describe() gives summary statistics for numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "print(\"üìà Statistical Summary:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-30",
   "metadata": {},
   "source": [
    "## Data Selection and Indexing\n",
    "\n",
    "Pandas offers multiple ways to select data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a single column\n",
    "departments = df['department']\n",
    "print(f\"üè¢ Department column type: {type(departments)}\")\n",
    "print(f\"üè¢ Unique departments: {departments.unique()}\")\n",
    "print(f\"üè¢ Department counts:\")\n",
    "print(departments.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select multiple columns\n",
    "student_basics = df[['student_id', 'department', 'years_in_program']]\n",
    "print(\"üë• Student basics:\")\n",
    "print(student_basics.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows based on conditions\n",
    "# Let's find the seasoned PhD students (4+ years)\n",
    "veterans = df[df['years_in_program'] >= 4]\n",
    "print(f\"üéì Veterans (4+ years): {len(veterans)} students\")\n",
    "print(f\"üìä Their average papers: {veterans['papers_published'].mean():.2f}\")\n",
    "\n",
    "# Multiple conditions\n",
    "ise_veterans = df[(df['years_in_program'] >= 4) & (df['department'].str.contains('ISE', na=False))]\n",
    "print(f\"\\nüè≠ ISE veterans: {len(ise_veterans)} students\")\n",
    "print(f\"üìä Their average papers: {ise_veterans['papers_published'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-34",
   "metadata": {},
   "source": [
    "## üéØ Mini Exercise: Department Analysis\n",
    "\n",
    "Let's analyze our departments, but first we need to deal with those inconsistent names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the department name mess\n",
    "print(\"üè¢ Department name variations:\")\n",
    "print(df['department'].value_counts())\n",
    "\n",
    "# Let's clean this up as we learn!\n",
    "def standardize_department(dept_name):\n",
    "    \"\"\"Clean up department names\"\"\"\n",
    "    if pd.isna(dept_name):\n",
    "        return dept_name # Keep NaN as is\n",
    "    \n",
    "    dept_name = str(dept_name).strip() # Remove leading/trailing whitespace\n",
    "    \n",
    "    if 'Industrial' in dept_name or dept_name in ['ISE', 'I.S.E.']:\n",
    "        return 'ISE'\n",
    "    elif 'Operations' in dept_name or dept_name in ['OR', 'O.R.']:\n",
    "        return 'OR'\n",
    "    elif 'Computer' in dept_name or dept_name in ['CSC', 'CS', 'Comp Sci']:\n",
    "        return 'CSC'\n",
    "    elif 'Electrical' in dept_name or dept_name in ['ECE', 'E.C.E.', 'EE']:\n",
    "        return 'ECE'\n",
    "    elif 'Mechanical' in dept_name or dept_name in ['MAE', 'M.A.E.']:\n",
    "        return 'MAE'\n",
    "    else:\n",
    "        return dept_name\n",
    "\n",
    "# Apply the cleaning function\n",
    "df['department_clean'] = df['department'].apply(standardize_department)\n",
    "\n",
    "print(\"\\n‚ú® Cleaned department names:\")\n",
    "print(df['department_clean'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-36",
   "metadata": {},
   "source": [
    "### ü§î Your Turn: Analyze by Department\n",
    "\n",
    "Now use the cleaned department data to answer these questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Find the answers to these questions:\n",
    "# 1. Which department has the highest average papers published?\n",
    "# 2. Which department spends the most time in Hunt Library on average?\n",
    "# 3. Which department has the highest stress levels?\n",
    "\n",
    "# Your code here:\n",
    "print(\"üìä Department Analysis:\")\n",
    "\n",
    "print(\"\\nüìÑ Average papers by department:\")\n",
    "papers_by_dept = df.groupby('department_clean')['papers_published'].mean().sort_values(ascending=False)\n",
    "print(papers_by_dept)\n",
    "\n",
    "print(\"\\nüìö Average Hunt Library hours by department:\")\n",
    "# Hint: This is very similar to the 'papers_by_dept' calculation.\n",
    "# Group by 'department_clean', select the 'hours_in_hunt_library_per_week' column,\n",
    "# calculate the mean, and then sort in descending order.\n",
    "hunt_by_dept = ???\n",
    "print(hunt_by_dept)\n",
    "\n",
    "print(\"\\nüò∞ Average stress level by department:\")\n",
    "# Hint: Group by 'department_clean' and calculate the mean of 'stress_level'.\n",
    "# This time, sort the results in ASCENDING order (True) to see the lowest stress levels first.\n",
    "# To find the HIGHEST stress levels, you'll need to look at the end of the sorted list!\n",
    "stress_by_dept = ???\n",
    "print(stress_by_dept)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-38",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Data Cleaning Workshop üßπ\n",
    "\n",
    "Now for the main event! Let's clean our messy dataset step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-39",
   "metadata": {},
   "source": [
    "## Step 1: Handle Missing Values\n",
    "\n",
    "Let's see what we're dealing with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"‚ùå Missing values by column:\")\n",
    "missing_summary = df.isnull().sum()\n",
    "missing_summary = missing_summary[missing_summary > 0]\n",
    "print(missing_summary)\n",
    "\n",
    "# Percentage of missing values\n",
    "print(\"\\nüìä Missing value percentages:\")\n",
    "missing_percentages = (df.isnull().sum() / len(df)) * 100\n",
    "missing_percentages = missing_percentages[missing_percentages > 0]\n",
    "print(missing_percentages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-41",
   "metadata": {},
   "source": [
    "### Different Strategies for Missing Values:\n",
    "\n",
    "1. **Remove rows** with missing values (if very few)\n",
    "2. **Fill with mean/median** (for numerical data)\n",
    "3. **Fill with mode** (for categorical data)\n",
    "4. **Forward/backward fill** (for time series)\n",
    "5. **Use sophisticated imputation** (advanced techniques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's handle missing values strategically\n",
    "\n",
    "# For coffee consumption: fill with median (some students might not drink coffee: coffee_cups_per_day)\n",
    "# Hint: Use `df['column'].median()` to get the median value.\n",
    "coffee_median = ???\n",
    "df['coffee_cups_per_day'] = df['coffee_cups_per_day'].fillna(coffee_median)\n",
    "print(f\"‚òï Filled missing coffee values with median: {coffee_median:.2f}\")\n",
    "\n",
    "# For stress level: fill with mean (stress is continuous: stress_level)\n",
    "# Hint: Same approach as above, but use mean instead of median.\n",
    "stress_mean = ???\n",
    "df['stress_level'] = df['stress_level'].fillna(stress_mean)\n",
    "print(f\"üò∞ Filled missing stress values with mean: {stress_mean:.2f}\")\n",
    "\n",
    "# For funding: fill with department-specific means\n",
    "# Hint: Group by 'department_clean', calculate the mean of 'funding_amount'.\n",
    "funding_by_dept = df.groupby('department_clean')['funding_amount'].mean()\n",
    "# Now fill missing funding amounts with the mean for that department using `map`.\n",
    "df['funding_amount'] = df['funding_amount'].fillna(\n",
    "    df['department_clean'].map(funding_by_dept)\n",
    ")\n",
    "print(f\"üí∞ Filled missing funding values with department-specific means\")\n",
    "\n",
    "# Check our work\n",
    "print(f\"\\n‚úÖ Missing values after cleaning: {df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-43",
   "metadata": {},
   "source": [
    "## Step 2: Remove Duplicates\n",
    "\n",
    "Duplicate rows can skew our analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "print(f\"üîÑ Duplicate rows: {df.duplicated().sum()}\")\n",
    "\n",
    "# Look at some duplicates\n",
    "if df.duplicated().sum() > 0:\n",
    "    print(\"\\nüëÄ Some duplicate rows:\")\n",
    "    duplicate_rows = df[df.duplicated(keep=False)].sort_values('student_id') # Show all duplicates\n",
    "    print(duplicate_rows[['student_id', 'department_clean', 'years_in_program', 'papers_published']].head(10))\n",
    "    \n",
    "    # Remove duplicates\n",
    "    df.drop_duplicates(inplace=True) # Keep only the first occurrence\n",
    "    print(f\"\\n‚úÖ Removed duplicates. New shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-45",
   "metadata": {},
   "source": [
    "## Step 3: Handle Outliers\n",
    "\n",
    "Let's find and deal with extreme values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for coffee outliers\n",
    "# Using quantile() to find Q1 and Q3\n",
    "coffee_q75 = df['coffee_cups_per_day'].quantile(0.75)\n",
    "coffee_q25 = df['coffee_cups_per_day'].quantile(0.25)\n",
    "coffee_iqr = coffee_q75 - coffee_q25\n",
    "\n",
    "# Define outliers as values beyond 1.5 * IQR from Q1 or Q3\n",
    "coffee_lower_bound = coffee_q25 - 1.5 * coffee_iqr\n",
    "coffee_upper_bound = coffee_q75 + 1.5 * coffee_iqr\n",
    "\n",
    "coffee_outliers = df[(df['coffee_cups_per_day'] < coffee_lower_bound) | # \"|\" is logical OR \n",
    "                    (df['coffee_cups_per_day'] > coffee_upper_bound)]\n",
    "\n",
    "print(f\"‚òï Coffee consumption outliers: {len(coffee_outliers)}\")\n",
    "print(f\"   Normal range: {coffee_lower_bound:.2f} - {coffee_upper_bound:.2f} cups/day\") \n",
    "print(f\"   Outliers range: {coffee_outliers['coffee_cups_per_day'].min():.2f} - {coffee_outliers['coffee_cups_per_day'].max():.2f} cups/day\")\n",
    "\n",
    "# Let's be reasonable and cap extreme values\n",
    "# Anyone drinking more than 10 cups/day might have data entry errors\n",
    "extreme_coffee = df['coffee_cups_per_day'] > 10\n",
    "print(f\"\\nüö® Students with >10 cups/day: {extreme_coffee.sum()}\")\n",
    "\n",
    "# Cap at 8 cups/day (still high but reasonable for a stressed PhD student)\n",
    "df.loc[df['coffee_cups_per_day'] > 8, 'coffee_cups_per_day'] = 8 # .loc is used for label-based indexing, while .iloc is for position-based indexing\n",
    "print(f\"‚úÖ Capped extreme coffee consumption at 8 cups/day\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-47",
   "metadata": {},
   "source": [
    "## Step 4: Feature Engineering\n",
    "\n",
    "Let's create some new features that might be useful for our analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a productivity score\n",
    "df['productivity_score'] = (df['papers_published'] * 2 + df['conferences_attended']) / df['years_in_program']\n",
    "\n",
    "# Create a work-life balance indicator\n",
    "# High Hunt Library hours + high stress = poor work-life balance\n",
    "df['work_life_balance'] = 10 - (df['hours_in_hunt_library_per_week'] / 10 + df['stress_level']) / 2\n",
    "df['work_life_balance'] = df['work_life_balance'].clip(1, 10)  # Using clip() to keep it between 1-10\n",
    "\n",
    "# Create a seniority category\n",
    "# Using cut() to bin years_in_program into categories\n",
    "df['seniority'] = pd.cut(df['years_in_program'], \n",
    "                        bins=[0, 2, 4, 10], \n",
    "                        labels=['Early', 'Mid', 'Advanced'])\n",
    "\n",
    "# Create a caffeine dependency category\n",
    "df['caffeine_level'] = pd.cut(df['coffee_cups_per_day'], \n",
    "                             bins=[0, 2, 4, 8], \n",
    "                             labels=['Low', 'Moderate', 'High'])\n",
    "\n",
    "print(\"üõ†Ô∏è New features created:\")\n",
    "print(f\"   - productivity_score: {df['productivity_score'].describe()}\")\n",
    "print(f\"   - work_life_balance: {df['work_life_balance'].describe()}\")\n",
    "print(f\"   - seniority: {df['seniority'].value_counts()}\")\n",
    "print(f\"   - caffeine_level: {df['caffeine_level'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-49",
   "metadata": {},
   "source": [
    "## üéØ Final Cleaning Exercise\n",
    "\n",
    "Your turn! Let's finish cleaning our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete these final cleaning steps:\n",
    "# 1. Replace the old 'department' column with 'department_clean'\n",
    "# 2. Round numerical columns to reasonable decimal places\n",
    "# 3. Reset the index after all our filtering\n",
    "# 4. Create a final summary of our cleaned dataset\n",
    "\n",
    "# Your code here:\n",
    "\n",
    "# 1. Replace department column\n",
    "df['department'] = ???\n",
    "df.drop('department_clean', axis=1, inplace=True)\n",
    "\n",
    "# 2. Round numerical columns\n",
    "df['coffee_cups_per_day'] = df['coffee_cups_per_day'].round(2) # Coffee rounded to 2 decimal places\n",
    "df['hours_in_hunt_library_per_week'] = df['hours_in_hunt_library_per_week'].round(1) # Hunt hours rounded to 1 decimal place\n",
    "df['stress_level'] = df['stress_level'].round(2)\n",
    "df['funding_amount'] = df['funding_amount'].round(2)\n",
    "df['distance_from_campus_miles'] = df['distance_from_campus_miles'].round(2)\n",
    "df['productivity_score'] = df['productivity_score'].round(3) # More precision for productivity\n",
    "\n",
    "# Round work-life balance to 2 decimal places\n",
    "df['work_life_balance'] = ???\n",
    "\n",
    "# 3. Reset index after filtering (the indices may be non-sequential now)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# 4. Final summary\n",
    "print(\"üéâ Final cleaned dataset summary:\")\n",
    "print(f\"   Shape: {df.shape}\")\n",
    "print(f\"   Missing values: {df.isnull().sum().sum()}\")\n",
    "print(f\"   Duplicates: {df.duplicated().sum()}\")\n",
    "print(f\"   Columns: {len(df.columns)}\")\n",
    "\n",
    "print(\"\\nüìä Final dataset preview:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-51",
   "metadata": {},
   "source": [
    "## Save Our Clean Dataset\n",
    "\n",
    "Let's save our cleaned dataset for use in Block 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "df.to_csv('phd_research_productivity_clean.csv', index=False)\n",
    "print(\"üíæ Clean dataset saved as 'phd_research_productivity_clean.csv'\")\n",
    "\n",
    "# Quick verification\n",
    "print(f\"\\n‚úÖ Verification - file size: {len(df)} rows, {len(df.columns)} columns\")\n",
    "print(f\"‚úÖ Ready for Block 3: Linear Regression Analysis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-53",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üéØ Bonus Section: Advanced Operations\n",
    "\n",
    "For those who finish early or want to dive deeper!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-54",
   "metadata": {},
   "source": [
    "## GroupBy Operations\n",
    "\n",
    "One of Pandas' most powerful features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by department and analyze\n",
    "dept_analysis = df.groupby('department').agg({\n",
    "    'papers_published': ['mean', 'std', 'max'],\n",
    "    'coffee_cups_per_day': 'mean',\n",
    "    'stress_level': 'mean',\n",
    "    'funding_amount': 'mean',\n",
    "    'productivity_score': 'mean'\n",
    "})\n",
    "\n",
    "print(\"üè¢ Department Analysis:\")\n",
    "print(dept_analysis.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-56",
   "metadata": {},
   "source": [
    "## Correlation Analysis\n",
    "\n",
    "Let's see what factors are related to research productivity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numerical columns for correlation\n",
    "numerical_cols = ['years_in_program', 'papers_published', 'conferences_attended',\n",
    "                  'coffee_cups_per_day', 'hours_in_hunt_library_per_week',\n",
    "                  'advisor_meetings_per_month', 'stress_level', 'funding_amount',\n",
    "                  'distance_from_campus_miles', 'productivity_score', 'work_life_balance']\n",
    "\n",
    "correlation_matrix = df[numerical_cols].corr()\n",
    "\n",
    "# Focus on correlations with papers_published\n",
    "papers_correlations = correlation_matrix['papers_published'].sort_values(ascending=False)\n",
    "\n",
    "print(\"üìä Correlations with Papers Published:\")\n",
    "for variable, correlation in papers_correlations.items():\n",
    "    if variable != 'papers_published':\n",
    "        print(f\"   {variable}: {correlation:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-58",
   "metadata": {},
   "source": [
    "## Data Visualization Preview\n",
    "\n",
    "A quick visualization to understand our data better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Papers by department\n",
    "df.groupby('department')['papers_published'].mean().plot(kind='bar', ax=axes[0,0])\n",
    "axes[0,0].set_title('Average Papers by Department')\n",
    "axes[0,0].set_ylabel('Papers Published')\n",
    "\n",
    "# Coffee vs Papers scatter plot\n",
    "axes[0,1].scatter(df['coffee_cups_per_day'], df['papers_published'], alpha=0.6)\n",
    "axes[0,1].set_xlabel('Coffee Cups per Day')\n",
    "axes[0,1].set_ylabel('Papers Published')\n",
    "axes[0,1].set_title('Coffee vs Research Output')\n",
    "\n",
    "# Stress level distribution\n",
    "df['stress_level'].hist(bins=20, ax=axes[1,0])\n",
    "axes[1,0].set_title('Stress Level Distribution')\n",
    "axes[1,0].set_xlabel('Stress Level')\n",
    "axes[1,0].set_ylabel('Frequency')\n",
    "\n",
    "# Years vs Papers\n",
    "axes[1,1].scatter(df['years_in_program'], df['papers_published'], alpha=0.6)\n",
    "axes[1,1].set_xlabel('Years in Program')\n",
    "axes[1,1].set_ylabel('Papers Published')\n",
    "axes[1,1].set_title('Experience vs Research Output')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìà Quick insights:\")\n",
    "print(f\"   - Average papers published: {df['papers_published'].mean():.2f}\")\n",
    "print(f\"   - Most productive department: {df.groupby('department')['papers_published'].mean().idxmax()}\")\n",
    "print(f\"   - Coffee-paper correlation: {df['coffee_cups_per_day'].corr(df['papers_published']):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-60",
   "metadata": {},
   "source": [
    "## üéØ Optional Exercise: Create Your Own Analysis\n",
    "\n",
    "Now it's your turn to explore! Pick one of these questions and write the code to answer it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose one of these questions and write code to answer it:\n",
    "\n",
    "# Question 1: Do students who live farther from campus have better work-life balance?\n",
    "# Question 2: Which combination of caffeine level and seniority is most productive?\n",
    "# Question 3: Is there an optimal number of advisor meetings per month for productivity?\n",
    "# Question 4: Create a \"PhD Success Score\" combining multiple factors\n",
    "\n",
    "# Your exploratory code here:\n",
    "\n",
    "# Example: Question 1 analysis\n",
    "print(\"üè† Distance vs Work-Life Balance Analysis:\")\n",
    "\n",
    "# Create distance categories\n",
    "df['distance_category'] = pd.cut(df['distance_from_campus_miles'],\n",
    "                                    bins=[0, 2, 5, 50],\n",
    "                                    labels=['Close (<2 miles)', 'Medium (2-5 miles)', 'Far (>5 miles)'])\n",
    "\n",
    "# Analyze work-life balance by distance\n",
    "# Explicitly set observed=False to retain current behavior and silence the FutureWarning\n",
    "distance_analysis = df.groupby('distance_category', observed=False)['work_life_balance'].agg(['mean', 'std', 'count'])\n",
    "print(distance_analysis)\n",
    "\n",
    "# Statistical test\n",
    "close_wlb = df[df['distance_category'] == 'Close (<2 miles)']['work_life_balance']\n",
    "far_wlb = df[df['distance_category'] == 'Far (>5 miles)']['work_life_balance']\n",
    "\n",
    "print(f\"\\nüìä Students living close to campus: {close_wlb.mean():.2f} avg work-life balance\")\n",
    "print(f\"üìä Students living far from campus: {far_wlb.mean():.2f} avg work-life balance\")\n",
    "print(f\"üìä Difference: {far_wlb.mean() - close_wlb.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-62",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üéâ Congratulations!\n",
    "\n",
    "## What We've Accomplished\n",
    "\n",
    "In this block, you've learned to:\n",
    "\n",
    "‚úÖ **NumPy Fundamentals**\n",
    "- Create and manipulate arrays\n",
    "- Perform vectorized operations\n",
    "- Calculate statistics efficiently\n",
    "- Understand why NumPy is faster than pure Python\n",
    "\n",
    "‚úÖ **Pandas Mastery**\n",
    "- Load and explore datasets\n",
    "- Select, filter, and manipulate data\n",
    "- Handle missing values strategically\n",
    "- Remove duplicates and outliers\n",
    "- Create new features from existing data\n",
    "\n",
    "‚úÖ **Real-World Data Cleaning**\n",
    "- Standardized inconsistent categorical data\n",
    "- Applied different missing value strategies\n",
    "- Created meaningful derived features\n",
    "- Prepared data for machine learning\n",
    "\n",
    "## Your Clean Dataset\n",
    "\n",
    "Our messy PhD student dataset is now clean and ready for analysis! In Block 3, we'll use this data to:\n",
    "- Build a linear regression model\n",
    "- Predict research productivity\n",
    "- Interpret model results\n",
    "- Understand what factors contribute to PhD success\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Real data is messy** - cleaning is often 80% of the work\n",
    "2. **NumPy and Pandas are complementary** - NumPy for computation, Pandas for data manipulation\n",
    "3. **Always explore your data first** - understand before you clean\n",
    "4. **Feature engineering matters** - new features can reveal hidden insights\n",
    "5. **Document your cleaning decisions** - others (including future you) need to understand your choices\n",
    "\n",
    "## Next Up: Block 3 üöÄ\n",
    "\n",
    "In **Block 3: From Data to Insights**, we'll take our clean dataset and:\n",
    "- Build predictive models with Scikit-learn\n",
    "- Understand linear regression concepts\n",
    "- Interpret model coefficients\n",
    "- Evaluate model performance\n",
    "- Make predictions about PhD student success!\n",
    "\n",
    "Great work, future data scientists! üéì‚ú®"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
